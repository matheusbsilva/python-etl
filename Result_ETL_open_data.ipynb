{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse tutorial vamos aprender o básico sobre as etapas de uma rotina de ETL(extract, transform and load). A ideia aqui é utilizar um dataset público real para demonstrar como implementar as diferentes etapas dessa rotina. Vamos explorar alguns conceitos básicos da Engenharia de dados, como implementar rotinas para extração de arquivos e como manipular dados com o Pandas. Lembrando que toda a implementação aqui é focada em *small data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ETL(Extract, transform and load)** é um processo que consiste em integrar dados de diferentes fontes buscando consolidá-los de uma maneira que facilite o processo de análise. O termo se popularizou com o surgimento das *data warehouses*, que nasceram da necessidade de centralizar diversas fontes de dados para permitir criar análises que ajudassem as empresas na tomada de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo consiste de três etapas:\n",
    "\n",
    "1. *Extract*: extrair dados de uma fonte de informações, seja banco de dados, API, arquivos, etc. Para serem processados posteriormente;\n",
    "2. *Transform*: nessa etapa os dados extraídos passam por uma transformação para atender os requisitos das aplicações cliente. A transormação envolve:\n",
    "    - Limpar e validar os dados para garantir qualidade;\n",
    "    - Transformar o formato dos dados para, por exemplo, facilitar usabilidade e busca;\n",
    "    - Combinar diferentes fontes para compor as informações necessárias;\n",
    "    - Aplicar regras de negócio.\n",
    "3. *Load*: carregar resultado das transformações no sistema de destino, como *date warehouses* ou *data lakes*.\n",
    "\n",
    "![](./img/etl_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data warehouse e data lake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data warehouses**, segundo Bill Inmon, é uma coleção de dados orientada à assunto, não volátil, integrada e variável no tempo para apoiar as decisões de negócio. Exemplos de ferramentas em nuvem:\n",
    "\n",
    "<div style=\"background-color: white\">\n",
    "  <img src=\"./img/redshift.png\" width=\"300px\" style=\"margin-right: 10rem;\">\n",
    "   <img src=\"./img/bigquery.png\" width=\"300px\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data lake** é um repositório que permite armazenar dados estruturados e não estruturados em qualquer escala. Opções de ferramentas para utilizar como storage para data lakes:\n",
    "\n",
    "<div style=\"background-color: white\">\n",
    "  <img src=\"./img/s3.png\" style=\"width: 200px; margin-right: 10rem;\">\n",
    "  <img src=\"./img/gcs.png\" style=\"width: 200px; margin-right: 10rem;\">\n",
    "  <img src=\"./img/azure_storage.png\" style=\"width: 300px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas \n",
    "\n",
    "Biblioteca Python open-source para análise e manipulação de dados. Começou a ser desenvolvida em 2008 e se tornou [open-source](https://github.com/pandas-dev/pandas) em 2009.\n",
    "\n",
    "![](./img/pandasgh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fonte de dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trabalhar com a fonte de dados abertos da [Comissão de Valores Mobiliários](http://www.cvm.gov.br/) contendo a cotação diária dos fundos de investimentos negociados no mercado brasileiro. Essa fonte é atualizada diariamente com os dados de fechamento do dia anterior e as cotações são agrupadas por arquivos correspondentes a cada mês do ano.\n",
    "\n",
    "Esse é um cenário bem comum em fontes de dados abertas, uma série de arquivos no formato CSV agrupando informações de acordo com a data, então a solução que vamos implementar durante o tutorial é reaproveitável para outras fontes de dados.\n",
    "\n",
    "Primeiro é importante analisar a fonte de dados, entender como ela está estruturada, quais campos compõem o dataset e como nós podemos automatizar a coleta dos dados.\n",
    "\n",
    "A fonte de dados contendo a cotação diária dos fundos pode ser acessada através do portal de dados abertos pelo link:\n",
    "\n",
    "http://www.dados.gov.br/dataset/fi-doc-inf_diario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de extração consiste nesse caso em fazer o download de todos os arquivos da janela de tempo q nos interessa. Para isso precisamos gerar o nome dos arquivos com as datas da janela de tempo no formato esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gera lista de datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dates(start, end, freq='M', format='%Y%m'):\n",
    "    return [date.strftime(format) for date in pd.date_range(start=start, end=end, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = generate_dates('2020-01', '2020-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisita e salva os arquivos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(date_list):\n",
    "    base_url = 'http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_{date}.csv'\n",
    "    dfs = [pd.read_csv(base_url.format(date=date), sep=';', parse_dates=['DT_COMPTC']) for date in date_list]\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extract = extract(date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vou abrir um parênteses para uma análise rápido do nosso dateframe, é claro que só a parte de exploração dos dados vale um tutorial completo, então não vou explorar muito esses aspecto aqui, mas de qualquer forma é importante ter noção de alguns pontos básicos quando se trabalha com uma fonte de dados:\n",
    "\n",
    "- Qual o tipo de cada coluna\n",
    "- Quantos valores nulos que existem\n",
    "- Como estão formatados\n",
    "\n",
    "Essa etapa não faz parte do ETL, na verdade essa exploração inicial normalmente é feita antes de construir a *pipeline* para entender a fonte de dados utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369894, 8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formato do dataframe (linhas, colunas)\n",
    "df_extract.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369894 entries, 0 to 369893\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   CNPJ_FUNDO     369894 non-null  object \n",
      " 1   DT_COMPTC      369894 non-null  object \n",
      " 2   VL_TOTAL       369894 non-null  float64\n",
      " 3   VL_QUOTA       369894 non-null  float64\n",
      " 4   VL_PATRIM_LIQ  369894 non-null  float64\n",
      " 5   CAPTC_DIA      369894 non-null  float64\n",
      " 6   RESG_DIA       369894 non-null  float64\n",
      " 7   NR_COTST       369894 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 22.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Quais são nossas colunas, seus tipos e se existem valores nulos\n",
    "df_extract.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de fundos: 17276\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de fundos:', df_extract.CNPJ_FUNDO.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNPJ_FUNDO</th>\n",
       "      <th>DT_COMPTC</th>\n",
       "      <th>VL_TOTAL</th>\n",
       "      <th>VL_QUOTA</th>\n",
       "      <th>VL_PATRIM_LIQ</th>\n",
       "      <th>CAPTC_DIA</th>\n",
       "      <th>RESG_DIA</th>\n",
       "      <th>NR_COTST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00.017.024/0001-53</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1132491.66</td>\n",
       "      <td>27.225023</td>\n",
       "      <td>1123583.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00.017.024/0001-53</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>1132685.12</td>\n",
       "      <td>27.224496</td>\n",
       "      <td>1123561.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00.017.024/0001-53</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>1132881.43</td>\n",
       "      <td>27.225564</td>\n",
       "      <td>1123605.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00.017.024/0001-53</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>1133076.85</td>\n",
       "      <td>27.226701</td>\n",
       "      <td>1123652.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00.017.024/0001-53</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>1132948.59</td>\n",
       "      <td>27.227816</td>\n",
       "      <td>1123698.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CNPJ_FUNDO   DT_COMPTC    VL_TOTAL   VL_QUOTA  VL_PATRIM_LIQ  \\\n",
       "0  00.017.024/0001-53  2020-01-02  1132491.66  27.225023     1123583.00   \n",
       "1  00.017.024/0001-53  2020-01-03  1132685.12  27.224496     1123561.25   \n",
       "2  00.017.024/0001-53  2020-01-06  1132881.43  27.225564     1123605.31   \n",
       "3  00.017.024/0001-53  2020-01-07  1133076.85  27.226701     1123652.24   \n",
       "4  00.017.024/0001-53  2020-01-08  1132948.59  27.227816     1123698.26   \n",
       "\n",
       "   CAPTC_DIA  RESG_DIA  NR_COTST  \n",
       "0        0.0       0.0         1  \n",
       "1        0.0       0.0         1  \n",
       "2        0.0       0.0         1  \n",
       "3        0.0       0.0         1  \n",
       "4        0.0       0.0         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa é necessário consolidar todas as informações extraídas, aplicar os tratamentos e regras de negócio que fazem sentido para a aplicação cliente, defini alguns objetivos para nos orientar durante essa etapa:\n",
    "\n",
    "1. Manter somente fundos com mais de `1000` cotistas;\n",
    "2. Mudar o formato do dataframe para:\n",
    "\n",
    "\n",
    "|            | 00.017.024/0001-53 | 97.929.213/0001-34 | 00.068.305/0001-35 | ... |\n",
    "|------------|--------------------|--------------------|--------------------|-----|\n",
    "| 2020-01-02 | 27.225023          | 27.112737          | 1.733476e+08       | ... |\n",
    "| 2020-01-03 | 27.224496          | 27.115661          | 6.611408e+07       | ... |\n",
    "| ...        | ...                | ...                | ...                | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_shareholders(df, qtd_shareholders = 1000):\n",
    "    df_final_date = df[df.DT_COMPTC == df.DT_COMPTC.max()]\n",
    "    valid_cnpjs = df_final_date.query('NR_COTST >= @qtd_shareholders').CNPJ_FUNDO\n",
    "    return raw_df.query('CNPJ_FUNDO in @valid_cnpjs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return (data.pipe(filter_by_shareholders)\n",
    "                .pipe(pd.pivot, index='DT_COMPTC', columns='CNPJ_FUNDO', values='VL_QUOTA'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para etapa de carregamento vamos exportar o resultado em formato CSV e também vou demostrar como subir esse arquivo no [S3 da Amazon](https://aws.amazon.com/pt/s3/), que é um serviço de armazenamento de objetos na cloud, comumente usado como *data lake*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bônus**: Essa função utiliza do SDK do AWS para fazer o upload do arquivo resultante em um *bucket* no S3 da AWS, não vou entrar em detalhes sobre a configuração dessa ferramenta, mas se tiver interesse em saber mais a [documentação do boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation) tem um passo a passo sobre como utilizar o SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def load(data, remote_file_name, bucket):\n",
    "    s3 = boto3.resource('s3')\n",
    "    return s3.Bucket(bucket).put_object(Key=remote_file_name, Body=data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_etl(since, to, s3_bucket):\n",
    "    date_list = generate_dates(since, to)\n",
    "    file_name = 'fi_info_diario_{}_{}.csv'.format(since, to)\n",
    "    \n",
    "    data_extract = extract(date_list)\n",
    "    data_transform = transform(data_extract)\n",
    "    return load(data_transform, file_name, s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_etl('2020-01-01', '2020-03-01', 'fretalks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o tutorial passamos por todas as etapas do processo ETL, claro que a solução que implementamos aqui é simples e trabalha com um pequeno volume de dados, a partir do momento em que o volume de dados aumenta é preciso buscar ferramentas otimizadas, mas o processo no geral continua o mesmo. E para levar essa *pipeline* para produção o que falta? \n",
    "\n",
    "Em produção é importante utilizar ferramentas que paralelizem a execução das tarefas e que também sejam capazes de lidar com falhas durante o processo, no caso de *pipelines* que realizam processamento em lote(como a que nós implementamos), temos por exemplo ferramentas como:\n",
    "- [Apache Airflow](https://airflow.apache.org/)\n",
    "- [Luigi](https://github.com/spotify/luigi)\n",
    "- [Apache Beam](https://beam.apache.org/)\n",
    "- [Prefect](https://www.prefect.io/)\n",
    "\n",
    "Exemplo de pipeline que utiliza dessa mesma fonte, e mais algumas, sendo executada no [Apache Airflow](https://airflow.apache.org/):\n",
    "\n",
    "![](img/airflow_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra quem quiser se informar(não é recomendação) sobre oportunidades de investimento de uma maneira mais simples, tem o grupo da Sharke no telegram:\n",
    "\n",
    "https://t.me/SharkeInvestimentos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
